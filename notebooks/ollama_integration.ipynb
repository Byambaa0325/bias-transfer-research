{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ollama Integration for Bias Transfer Research\n",
        "\n",
        "This notebook demonstrates how to use Ollama (self-hosted LLM inference server) for model evaluation in bias transfer research.\n",
        "\n",
        "## Overview\n",
        "\n",
        "Ollama is a user-friendly LLM inference server that provides:\n",
        "- ✅ **Native Windows support** (unlike vLLM)\n",
        "- Easy installation and setup\n",
        "- Local model management\n",
        "- OpenAI-compatible API\n",
        "- Support for quantized models\n",
        "- Good performance on consumer GPUs\n",
        "\n",
        "## Why Ollama?\n",
        "\n",
        "- **Better Windows Support**: Works natively on Windows (no WSL2 needed)\n",
        "- **Easier Setup**: Simple installation, automatic model downloads\n",
        "- **Good for RTX 4060**: Optimized for consumer GPUs\n",
        "- **Model Library**: Pre-configured models ready to use\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Hardware**: NVIDIA GPU with CUDA support (RTX 4060 recommended)\n",
        "2. **Software**: \n",
        "   - **Windows**: Ollama for Windows (download from ollama.ai)\n",
        "   - **Linux/macOS**: Ollama CLI\n",
        "3. **Models**: Ollama model IDs (e.g., `llama3.1:8b`, `mistral:7b`)\n",
        "\n",
        "## Setup Steps\n",
        "\n",
        "1. **Install Ollama**: Download from https://ollama.ai (Windows) or `curl -fsSL https://ollama.ai/install.sh | sh` (Linux/macOS)\n",
        "2. **Pull a model**: `ollama pull llama3.1:8b`\n",
        "3. **Start Ollama server**: Usually runs automatically after installation\n",
        "4. **Use this notebook** to evaluate models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n",
        "!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Platform: Windows 10\n",
            "Python: 3.11.14\n",
            "\n",
            "✗ Ollama not installed or not in PATH\n",
            "\n",
            "Installation instructions:\n",
            "  1. Download from: https://ollama.ai/download\n",
            "  2. Run the installer\n",
            "  3. Ollama will start automatically\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# Check platform\n",
        "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "\n",
        "# Check if Ollama is installed\n",
        "try:\n",
        "    result = subprocess.run(['ollama', '--version'], capture_output=True, text=True, timeout=5)\n",
        "    if result.returncode == 0:\n",
        "        print(f\"\\n✓ Ollama installed: {result.stdout.strip()}\")\n",
        "        ollama_installed = True\n",
        "    else:\n",
        "        print(\"\\n✗ Ollama not found in PATH\")\n",
        "        ollama_installed = False\n",
        "except (FileNotFoundError, subprocess.TimeoutExpired):\n",
        "    print(\"\\n✗ Ollama not installed or not in PATH\")\n",
        "    ollama_installed = False\n",
        "    print(\"\\nInstallation instructions:\")\n",
        "    if platform.system() == \"Windows\":\n",
        "        print(\"  1. Download from: https://ollama.ai/download\")\n",
        "        print(\"  2. Run the installer\")\n",
        "        print(\"  3. Ollama will start automatically\")\n",
        "    else:\n",
        "        print(\"  Run: curl -fsSL https://ollama.ai/install.sh | sh\")\n",
        "\n",
        "# Check if Ollama server is running\n",
        "if ollama_installed:\n",
        "    try:\n",
        "        import requests\n",
        "        try:\n",
        "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
        "            if response.status_code == 200:\n",
        "                print(\"\\n✓ Ollama server is running\")\n",
        "                models = response.json().get('models', [])\n",
        "                if models:\n",
        "                    print(f\"✓ Found {len(models)} model(s):\")\n",
        "                    for model in models[:5]:  # Show first 5\n",
        "                        print(f\"  - {model.get('name', 'unknown')}\")\n",
        "                else:\n",
        "                    print(\"⚠ No models installed. Pull a model with: ollama pull llama3.1:8b\")\n",
        "            else:\n",
        "                print(f\"\\n⚠ Ollama server returned status {response.status_code}\")\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            print(\"\\n✗ Ollama server is NOT running\")\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"STARTING OLLAMA SERVER...\")\n",
        "            print(\"=\"*70)\n",
        "            \n",
        "            if platform.system() == \"Windows\":\n",
        "                print(\"\\nOn Windows, Ollama should run as a service.\")\n",
        "                print(\"Trying to start it...\")\n",
        "                try:\n",
        "                    # Try to start Ollama service on Windows\n",
        "                    subprocess.run(['ollama', 'serve'], check=False, timeout=1)\n",
        "                    print(\"Started 'ollama serve' command\")\n",
        "                    print(\"Waiting 3 seconds for server to start...\")\n",
        "                    time.sleep(3)\n",
        "                    \n",
        "                    # Check again\n",
        "                    try:\n",
        "                        response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
        "                        if response.status_code == 200:\n",
        "                            print(\"✓ Ollama server is now running!\")\n",
        "                        else:\n",
        "                            print(\"⚠ Server started but not responding correctly\")\n",
        "                    except:\n",
        "                        print(\"\\n⚠ Server may still be starting. Try:\")\n",
        "                        print(\"  1. Check Windows Services (services.msc) for 'Ollama'\")\n",
        "                        print(\"  2. Or run manually: ollama serve\")\n",
        "                        print(\"  3. Or restart Ollama from Start Menu\")\n",
        "                except Exception as e:\n",
        "                    print(f\"\\n⚠ Could not start server automatically: {e}\")\n",
        "                    print(\"\\nManual steps:\")\n",
        "                    print(\"  1. Open Command Prompt or PowerShell\")\n",
        "                    print(\"  2. Run: ollama serve\")\n",
        "                    print(\"  3. Keep that window open\")\n",
        "                    print(\"  4. Come back to this notebook\")\n",
        "            else:\n",
        "                print(\"\\nOn Linux/macOS, start Ollama with:\")\n",
        "                print(\"  ollama serve\")\n",
        "                print(\"\\nOr run it in the background:\")\n",
        "                print(\"  nohup ollama serve > /dev/null 2>&1 &\")\n",
        "                \n",
        "    except ImportError:\n",
        "        print(\"⚠ Could not check server status (requests not installed)\")\n",
        "        print(\"  Install with: pip install requests\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n⚠ Error checking server: {e}\")\n",
        "        print(\"\\nTo start Ollama server manually:\")\n",
        "        if platform.system() == \"Windows\":\n",
        "            print(\"  1. Open Command Prompt or PowerShell\")\n",
        "            print(\"  2. Run: ollama serve\")\n",
        "            print(\"  3. Keep that window open\")\n",
        "        else:\n",
        "            print(\"  Run: ollama serve\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1. Start Ollama Server (if not running)\n",
        "\n",
        "If the server check above showed it's not running, use this cell to start it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to start Ollama server...\n",
            "======================================================================\n",
            "\n",
            "On Windows, you have two options:\n",
            "\n",
            "Option 1: Start as background process (recommended)\n",
            "  - Open a NEW Command Prompt or PowerShell window\n",
            "  - Run: ollama serve\n",
            "  - Keep that window open\n",
            "\n",
            "Option 2: Check Windows Service\n",
            "  - Press Win+R, type: services.msc\n",
            "  - Look for 'Ollama' service\n",
            "  - Right-click and select 'Start' if it's stopped\n",
            "\n",
            "After starting, run the cell above again to verify.\n",
            "\n",
            "Trying to start 'ollama serve' in background...\n",
            "⚠ Cannot start in background from notebook on Windows.\n",
            "   Please start it manually using the instructions above.\n"
          ]
        }
      ],
      "source": [
        "# Start Ollama server\n",
        "import subprocess\n",
        "import platform\n",
        "import time\n",
        "import requests\n",
        "\n",
        "print(\"Attempting to start Ollama server...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if platform.system() == \"Windows\":\n",
        "    print(\"\\nOn Windows, you have two options:\")\n",
        "    print(\"\\nOption 1: Start as background process (recommended)\")\n",
        "    print(\"  - Open a NEW Command Prompt or PowerShell window\")\n",
        "    print(\"  - Run: ollama serve\")\n",
        "    print(\"  - Keep that window open\")\n",
        "    print(\"\\nOption 2: Check Windows Service\")\n",
        "    print(\"  - Press Win+R, type: services.msc\")\n",
        "    print(\"  - Look for 'Ollama' service\")\n",
        "    print(\"  - Right-click and select 'Start' if it's stopped\")\n",
        "    print(\"\\nAfter starting, run the cell above again to verify.\")\n",
        "    \n",
        "    # Try to start it anyway (might work)\n",
        "    try:\n",
        "        print(\"\\nTrying to start 'ollama serve' in background...\")\n",
        "        # On Windows, we can't easily run it in background from notebook\n",
        "        # So we'll just provide instructions\n",
        "        print(\"⚠ Cannot start in background from notebook on Windows.\")\n",
        "        print(\"   Please start it manually using the instructions above.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "else:\n",
        "    # Linux/macOS - can start in background\n",
        "    try:\n",
        "        print(\"Starting Ollama server in background...\")\n",
        "        process = subprocess.Popen(\n",
        "            ['ollama', 'serve'],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE\n",
        "        )\n",
        "        print(\"✓ Started Ollama server process\")\n",
        "        print(\"Waiting 3 seconds for server to initialize...\")\n",
        "        time.sleep(3)\n",
        "        \n",
        "        # Check if it's running\n",
        "        try:\n",
        "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
        "            if response.status_code == 200:\n",
        "                print(\"✓ Ollama server is now running!\")\n",
        "            else:\n",
        "                print(\"⚠ Server started but not responding correctly\")\n",
        "        except:\n",
        "            print(\"⚠ Server may still be starting. Wait a few more seconds.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error starting server: {e}\")\n",
        "        print(\"\\nTry running manually:\")\n",
        "        print(\"  ollama serve\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Ollama Client Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional, List\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "import requests\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Configuration\n",
        "OLLAMA_API_BASE = os.getenv(\"OLLAMA_API_BASE\", \"http://localhost:11434\")\n",
        "\n",
        "print(f\"Ollama API Base: {OLLAMA_API_BASE}\")\n",
        "print(f\"Project root: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Ollama Client Adapter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OllamaClient:\n",
        "    \"\"\"\n",
        "    Client adapter for Ollama API.\n",
        "    \n",
        "    Ollama provides a REST API for model inference.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, api_base: str = None):\n",
        "        \"\"\"\n",
        "        Initialize Ollama client.\n",
        "        \n",
        "        Args:\n",
        "            api_base: Ollama API base URL (default: http://localhost:11434)\n",
        "        \"\"\"\n",
        "        self.api_base = api_base or OLLAMA_API_BASE\n",
        "        \n",
        "        # Test connection\n",
        "        try:\n",
        "            response = requests.get(f\"{self.api_base}/api/tags\", timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                print(f\"✓ Ollama client initialized: {self.api_base}\")\n",
        "            else:\n",
        "                raise ValueError(f\"Ollama server returned status {response.status_code}\")\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            raise ValueError(\n",
        "                f\"Could not connect to Ollama server at {self.api_base}\\n\"\n",
        "                \"Make sure Ollama is running. On Windows, it should start automatically.\\n\"\n",
        "                \"On Linux/macOS, start with: ollama serve\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Failed to initialize Ollama client: {e}\")\n",
        "    \n",
        "    def invoke(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        model: str,\n",
        "        max_tokens: int = 500,\n",
        "        temperature: Optional[float] = None,\n",
        "        stop_sequences: Optional[List[str]] = None,\n",
        "        **kwargs\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Invoke Ollama model.\n",
        "        \n",
        "        Args:\n",
        "            messages: Conversation messages\n",
        "            model: Model name (e.g., \"llama3.1:8b\")\n",
        "            max_tokens: Maximum tokens in response (Ollama uses \"num_predict\")\n",
        "            temperature: Sampling temperature\n",
        "            stop_sequences: Stop sequences\n",
        "            **kwargs: Additional parameters\n",
        "            \n",
        "        Returns:\n",
        "            Response dict with 'content' key containing list of dicts with 'text'\n",
        "        \"\"\"\n",
        "        # Prepare parameters\n",
        "        params = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"options\": {\n",
        "                \"num_predict\": max_tokens,  # Ollama uses num_predict instead of max_tokens\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        if temperature is not None:\n",
        "            params[\"options\"][\"temperature\"] = temperature\n",
        "        \n",
        "        if stop_sequences:\n",
        "            params[\"options\"][\"stop\"] = stop_sequences\n",
        "        \n",
        "        # Add any additional options\n",
        "        if kwargs:\n",
        "            params[\"options\"].update(kwargs)\n",
        "        \n",
        "        try:\n",
        "            response = requests.post(\n",
        "                f\"{self.api_base}/api/chat\",\n",
        "                json=params,\n",
        "                timeout=120\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            result = response.json()\n",
        "            \n",
        "            # Extract text from response\n",
        "            text = result.get(\"message\", {}).get(\"content\", \"\")\n",
        "            \n",
        "            # Format to match Bedrock response structure\n",
        "            return {\n",
        "                \"content\": [{\"text\": text}],\n",
        "                \"metadata\": {\n",
        "                    \"model\": model,\n",
        "                    \"usage\": {\n",
        "                        \"prompt_tokens\": result.get(\"prompt_eval_count\"),\n",
        "                        \"completion_tokens\": result.get(\"eval_count\"),\n",
        "                        \"total_tokens\": result.get(\"prompt_eval_count\", 0) + result.get(\"eval_count\", 0),\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            raise Exception(f\"Ollama API call failed: {e}\")\n",
        "    \n",
        "    def list_models(self) -> List[str]:\n",
        "        \"\"\"List available models on Ollama server.\"\"\"\n",
        "        try:\n",
        "            response = requests.get(f\"{self.api_base}/api/tags\", timeout=5)\n",
        "            response.raise_for_status()\n",
        "            models_data = response.json()\n",
        "            return [model[\"name\"] for model in models_data.get(\"models\", [])]\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not list models: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def pull_model(self, model_name: str):\n",
        "        \"\"\"\n",
        "        Pull a model from Ollama library.\n",
        "        \n",
        "        Args:\n",
        "            model_name: Model name (e.g., \"llama3.1:8b\")\n",
        "        \"\"\"\n",
        "        print(f\"Pulling model: {model_name}\")\n",
        "        print(\"This may take a while depending on model size...\")\n",
        "        \n",
        "        try:\n",
        "            response = requests.post(\n",
        "                f\"{self.api_base}/api/pull\",\n",
        "                json={\"name\": model_name},\n",
        "                stream=True,\n",
        "                timeout=300\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            # Stream the response\n",
        "            for line in response.iter_lines():\n",
        "                if line:\n",
        "                    try:\n",
        "                        data = json.loads(line)\n",
        "                        if \"status\" in data:\n",
        "                            print(f\"  {data['status']}\")\n",
        "                    except:\n",
        "                        pass\n",
        "            \n",
        "            print(f\"✓ Model {model_name} pulled successfully\")\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Failed to pull model: {e}\")\n",
        "\n",
        "# Test connection\n",
        "try:\n",
        "    ollama_client = OllamaClient()\n",
        "    models = ollama_client.list_models()\n",
        "    if models:\n",
        "        print(f\"\\n✓ Available models: {models}\")\n",
        "    else:\n",
        "        print(\"\\n⚠ No models installed.\")\n",
        "        print(\"  Pull a model with: ollama pull llama3.1:8b\")\n",
        "        print(\"  Or use: ollama_client.pull_model('llama3.1:8b')\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Could not connect to Ollama server: {e}\")\n",
        "    print(f\"  Make sure Ollama is running at {OLLAMA_API_BASE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Ollama Evaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "class OllamaEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluator for Ollama models.\n",
        "    \n",
        "    Similar to ModelEvaluator but uses Ollama client instead of Bedrock.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_id: str, ollama_client: OllamaClient = None):\n",
        "        \"\"\"\n",
        "        Initialize evaluator for a specific Ollama model.\n",
        "        \n",
        "        Args:\n",
        "            model_id: Model ID (e.g., \"llama3.1:8b\")\n",
        "            ollama_client: Ollama client instance (creates new one if not provided)\n",
        "        \"\"\"\n",
        "        self.model_id = model_id\n",
        "        self.client = ollama_client or OllamaClient()\n",
        "        \n",
        "    def evaluate_entry(\n",
        "        self,\n",
        "        entry: pd.Series,\n",
        "        bias_type: str\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate a single entry with a specific bias type.\n",
        "        \n",
        "        Args:\n",
        "            entry: Dataset row (pandas Series)\n",
        "            bias_type: Bias type to evaluate\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with evaluation results\n",
        "        \"\"\"\n",
        "        turn1_question_col = f'turn1_question_{bias_type}'\n",
        "        target_question = entry['target_question']\n",
        "        turn1_question = entry.get(turn1_question_col)\n",
        "        \n",
        "        result = {\n",
        "            'entry_index': entry.name,\n",
        "            'model_id': self.model_id,\n",
        "            'bias_type': bias_type,\n",
        "            'target_question': target_question,\n",
        "            'turn1_question': turn1_question,\n",
        "            # Include original EMGSD data\n",
        "            'emgsd_text': entry.get('emgsd_text', ''),\n",
        "            'emgsd_stereotype_type': entry.get('emgsd_stereotype_type', ''),\n",
        "            'emgsd_category': entry.get('emgsd_category', ''),\n",
        "            'emgsd_trait': entry.get('emgsd_trait', ''),\n",
        "            'emgsd_target_word': entry.get('emgsd_target_word', ''),\n",
        "        }\n",
        "        \n",
        "        # Skip if no turn1 question\n",
        "        if pd.isna(turn1_question) or not turn1_question:\n",
        "            result['error'] = 'No turn1 question available'\n",
        "            result['turn1_response'] = None\n",
        "            result['turn2_response'] = None\n",
        "            result['control_response'] = None\n",
        "            return result\n",
        "        \n",
        "        # System prompt for target question augmentation\n",
        "        system_prompt_turn2 = \"Please augment the following phrase into short sentence of up to 10 word. You MUST only return 1 sentence.\"\n",
        "        target_question_with_system = f\"{system_prompt_turn2}\\n\\n{target_question}\"\n",
        "        \n",
        "        # Biased conversation: Turn 1 + Turn 2\n",
        "        turn1_response = self._get_response(turn1_question)\n",
        "        turn2_response = self._get_response(target_question_with_system, conversation_history=[\n",
        "            {\"role\": \"user\", \"content\": turn1_question},\n",
        "            {\"role\": \"assistant\", \"content\": turn1_response}\n",
        "        ])\n",
        "        \n",
        "        # Control: Just target question (no bias) - also with system prompt\n",
        "        control_response = self._get_response(target_question_with_system)\n",
        "        \n",
        "        result['turn1_response'] = turn1_response\n",
        "        result['turn2_response'] = turn2_response\n",
        "        result['control_response'] = control_response\n",
        "        result['error'] = None\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def _get_response(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        conversation_history: Optional[List[Dict[str, str]]] = None\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Get response from the Ollama model.\n",
        "        \n",
        "        Args:\n",
        "            prompt: User prompt\n",
        "            conversation_history: Previous conversation (for multi-turn)\n",
        "            \n",
        "        Returns:\n",
        "            Model response text\n",
        "        \"\"\"\n",
        "        if conversation_history:\n",
        "            # Multi-turn conversation\n",
        "            messages = conversation_history + [{\"role\": \"user\", \"content\": prompt}]\n",
        "        else:\n",
        "            # Single turn\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        \n",
        "        # Get stop sequences for Llama models\n",
        "        stop_sequences = None\n",
        "        if 'llama' in self.model_id.lower() or 'meta' in self.model_id.lower():\n",
        "            stop_sequences = [\n",
        "                \"\\n\\nUser:\",\n",
        "                \"\\n\\nAssistant:\",\n",
        "                \"\\nUser:\",\n",
        "                \"\\nAssistant:\",\n",
        "            ]\n",
        "        \n",
        "        # Get response\n",
        "        response = self.client.invoke(\n",
        "            messages=messages,\n",
        "            model=self.model_id,\n",
        "            max_tokens=500,\n",
        "            stop_sequences=stop_sequences,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        \n",
        "        # Extract text\n",
        "        if isinstance(response, dict):\n",
        "            try:\n",
        "                text = response[\"content\"][0][\"text\"]\n",
        "                # Post-process to remove repetitive loops\n",
        "                text = self._truncate_repetitive_loops(text)\n",
        "                return text\n",
        "            except (KeyError, IndexError, TypeError):\n",
        "                # Fallback extraction\n",
        "                content = response.get(\"content\", [])\n",
        "                if isinstance(content, list) and len(content) > 0:\n",
        "                    first_item = content[0]\n",
        "                    if isinstance(first_item, dict):\n",
        "                        text = first_item.get(\"text\", \"\")\n",
        "                        text = self._truncate_repetitive_loops(text)\n",
        "                        return text\n",
        "                    elif isinstance(first_item, str):\n",
        "                        text = first_item\n",
        "                        text = self._truncate_repetitive_loops(text)\n",
        "                        return text\n",
        "        \n",
        "        raise Exception(f\"Could not extract text from response: {response}\")\n",
        "    \n",
        "    def _truncate_repetitive_loops(self, text: str, max_length: int = 1000) -> str:\n",
        "        \"\"\"\n",
        "        Detect and truncate repetitive loops in model responses.\n",
        "        \n",
        "        Args:\n",
        "            text: Raw model response\n",
        "            max_length: Maximum character length before truncation\n",
        "            \n",
        "        Returns:\n",
        "            Cleaned text with loops removed\n",
        "        \"\"\"\n",
        "        if not text or len(text) < 50:\n",
        "            return text\n",
        "        \n",
        "        # Truncate at conversation boundaries\n",
        "        user_patterns = [\n",
        "            r'\\n\\nUser:.*$',\n",
        "            r'\\nUser:.*$',\n",
        "            r'\\n\\nAssistant:.*$',\n",
        "            r'\\nAssistant:.*$',\n",
        "        ]\n",
        "        \n",
        "        for pattern in user_patterns:\n",
        "            match = re.search(pattern, text, re.MULTILINE | re.IGNORECASE)\n",
        "            if match:\n",
        "                text = text[:match.start()].strip()\n",
        "                break\n",
        "        \n",
        "        # Hard limit on length\n",
        "        if len(text) > max_length:\n",
        "            truncated = text[:max_length]\n",
        "            last_period = truncated.rfind('.')\n",
        "            if last_period > max_length * 0.7:\n",
        "                text = text[:last_period + 1]\n",
        "            else:\n",
        "                text = truncated + \"...\"\n",
        "        \n",
        "        return text.strip()\n",
        "\n",
        "print(\"✓ OllamaEvaluator class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the multi-turn EMGSD dataset\n",
        "dataset_path = project_root / \"dataset_generation\" / \"data\"\n",
        "\n",
        "# Find latest dataset file\n",
        "if dataset_path.is_dir():\n",
        "    dataset_files = list(dataset_path.glob(\"multiturn_emgsd_dataset_*.csv\"))\n",
        "    if dataset_files:\n",
        "        dataset_path = max(dataset_files, key=lambda p: p.stat().st_mtime)\n",
        "        print(f\"✓ Found latest dataset: {dataset_path.name}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No dataset files found in {dataset_path}\")\n",
        "\n",
        "df = pd.read_csv(dataset_path)\n",
        "print(f\"✓ Loaded dataset: {len(df):,} entries\")\n",
        "print(f\"✓ Columns: {len(df.columns)} columns\")\n",
        "print(f\"\\nFirst few columns: {list(df.columns[:10])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "# Common Ollama models:\n",
        "# - llama3.1:8b (Llama 3.1 8B)\n",
        "# - llama3.2:3b (Llama 3.2 3B)\n",
        "# - mistral:7b (Mistral 7B)\n",
        "# - qwen2.5:7b (Qwen 2.5 7B)\n",
        "OLLAMA_MODEL_ID = \"llama3.1:8b\"  # Change to your model\n",
        "\n",
        "# Evaluation configuration\n",
        "SAMPLE_LIMIT = 10  # Start with small sample for testing\n",
        "BIAS_TYPES = [\"confirmation_bias\", \"anchoring_bias\"]  # Start with 2 bias types\n",
        "\n",
        "# Output directory\n",
        "output_dir = project_root / \"model_evaluations\" / \"ollama_results\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Model: {OLLAMA_MODEL_ID}\")\n",
        "print(f\"Sample limit: {SAMPLE_LIMIT}\")\n",
        "print(f\"Bias types: {BIAS_TYPES}\")\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "\n",
        "# Check if model is available\n",
        "try:\n",
        "    client = OllamaClient()\n",
        "    available_models = client.list_models()\n",
        "    if OLLAMA_MODEL_ID not in available_models:\n",
        "        print(f\"\\n⚠ Model '{OLLAMA_MODEL_ID}' not found in available models.\")\n",
        "        print(f\"Available models: {available_models}\")\n",
        "        print(f\"\\nTo pull the model, run:\")\n",
        "        print(f\"  ollama pull {OLLAMA_MODEL_ID}\")\n",
        "        print(f\"Or use: client.pull_model('{OLLAMA_MODEL_ID}')\")\n",
        "    else:\n",
        "        print(f\"\\n✓ Model '{OLLAMA_MODEL_ID}' is available\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n⚠ Could not check model availability: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = OllamaEvaluator(OLLAMA_MODEL_ID)\n",
        "\n",
        "# Limit dataset\n",
        "df_sample = df.head(SAMPLE_LIMIT)\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"EVALUATING: {OLLAMA_MODEL_ID}\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Entries: {len(df_sample)}\")\n",
        "print(f\"Bias types: {len(BIAS_TYPES)}\")\n",
        "print(f\"Total evaluations: {len(df_sample) * len(BIAS_TYPES)}\")\n",
        "print()\n",
        "\n",
        "# Evaluate each entry and bias type\n",
        "for idx, entry in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Entries\"):\n",
        "    for bias_type in BIAS_TYPES:\n",
        "        try:\n",
        "            result = evaluator.evaluate_entry(entry, bias_type)\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"\\n✗ Error evaluating entry {idx}, bias {bias_type}: {e}\")\n",
        "            result = {\n",
        "                'entry_index': idx,\n",
        "                'model_id': OLLAMA_MODEL_ID,\n",
        "                'bias_type': bias_type,\n",
        "                'error': str(e)\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "print(f\"\\n✓ Completed {len(results)} evaluations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results as JSON\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_name_safe = OLLAMA_MODEL_ID.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
        "output_file = output_dir / f\"evaluation_{model_name_safe}_{timestamp}.json\"\n",
        "\n",
        "output_data = {\n",
        "    \"model_id\": OLLAMA_MODEL_ID,\n",
        "    \"timestamp\": timestamp,\n",
        "    \"sample_limit\": SAMPLE_LIMIT,\n",
        "    \"bias_types\": BIAS_TYPES,\n",
        "    \"total_evaluations\": len(results),\n",
        "    \"results\": results\n",
        "}\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"✓ Saved results to: {output_file}\")\n",
        "print(f\"✓ Total results: {len(results)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Quick Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrame for analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"QUICK ANALYSIS\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Success rate\n",
        "successful = results_df[results_df['error'].isna()]\n",
        "failed = results_df[results_df['error'].notna()]\n",
        "\n",
        "print(f\"\\nSuccess rate: {len(successful)}/{len(results_df)} ({100*len(successful)/len(results_df):.1f}%)\")\n",
        "\n",
        "if len(failed) > 0:\n",
        "    print(f\"\\nFailed evaluations: {len(failed)}\")\n",
        "    print(failed[['entry_index', 'bias_type', 'error']].head())\n",
        "\n",
        "# Response length statistics\n",
        "if len(successful) > 0:\n",
        "    successful['turn2_length'] = successful['turn2_response'].str.len()\n",
        "    successful['control_length'] = successful['control_response'].str.len()\n",
        "    \n",
        "    print(f\"\\nResponse length statistics:\")\n",
        "    print(f\"Turn 2 (biased) - Mean: {successful['turn2_length'].mean():.1f}, Median: {successful['turn2_length'].median():.1f}\")\n",
        "    print(f\"Control - Mean: {successful['control_length'].mean():.1f}, Median: {successful['control_length'].median():.1f}\")\n",
        "\n",
        "# Show sample responses\n",
        "if len(successful) > 0:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"SAMPLE RESPONSES\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    sample = successful.iloc[0]\n",
        "    print(f\"\\nEntry: {sample['entry_index']}\")\n",
        "    print(f\"Bias type: {sample['bias_type']}\")\n",
        "    print(f\"\\nTurn 1 question: {sample['turn1_question'][:100]}...\")\n",
        "    print(f\"\\nTurn 2 response (biased): {sample['turn2_response'][:200]}...\")\n",
        "    print(f\"\\nControl response: {sample['control_response'][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Results\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
